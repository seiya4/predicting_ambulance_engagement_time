{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddff757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import logging, sys\n",
    "logging.disable(sys.maxsize)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from multi_freq_ldpy.pure_frequency_oracles.GRR import GRR_Client\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dfda0b",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f71c9f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"RF_Base\"\n",
    "target = \"ind_avg\"\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec0b76",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00704633",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36a9e9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (106520, 34)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset_classification.csv\")\n",
    "df[\"alert\"] = pd.to_datetime(df[\"alert\"])\n",
    "df.sort_values(by=\"alert\", ascending=True, inplace=True)\n",
    "print(\"shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f2d990",
   "metadata": {},
   "source": [
    "## Dividing into learn and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b576fb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90759, 33) (15761, 33)\n"
     ]
    }
   ],
   "source": [
    "learn = df.loc[df[\"alert\"]<datetime(2021,7,1,0,0,0)]\n",
    "test = df.loc[df[\"alert\"]>=datetime(2021,7,1,0,0,0)]\n",
    "del learn[\"alert\"], test[\"alert\"]\n",
    "print(learn.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c406ba2",
   "metadata": {},
   "source": [
    "## Applying Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be66a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "binaryLabelDataset = BinaryLabelDataset(favorable_label=0,\n",
    "                                        unfavorable_label=1,\n",
    "                                        df=learn,\n",
    "                                        label_names=['ind_avg'],\n",
    "                                        protected_attribute_names=['ind_prof'])\n",
    "privileged_groups_params = dict(\n",
    "    privileged_groups=[{\"ind_prof\": 1}],\n",
    "    unprivileged_groups=[{\"ind_prof\": 0}]\n",
    ")\n",
    "rew = Reweighing(**privileged_groups_params)\n",
    "ds_transf_train = rew.fit_transform(binaryLabelDataset)\n",
    "pickle.dump(ds_transf_train, open(folder_name + \"/\" + \"ds_transf_train.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b032330",
   "metadata": {},
   "source": [
    "## Dividing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54ed4ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = learn.drop(target, axis=1)\n",
    "y_train = learn[target]\n",
    "X_test = test.drop(target, axis=1)\n",
    "y_test = test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c6fe8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_dataset = {}\n",
    "dico_dataset[\"X_train\"] = X_train\n",
    "dico_dataset[\"y_train\"] = y_train\n",
    "dico_dataset[\"X_test\"] = X_test\n",
    "dico_dataset[\"y_test\"] = y_test\n",
    "pickle.dump(dico_dataset, open(folder_name + \"/\" + \"train_test_xy.dat\", \"wb\"))\n",
    "del dico_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8d4fc",
   "metadata": {},
   "source": [
    "# Analysis of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa729314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 75.06\n",
      "accuracy class 0: 70.68 accuracy class 1: 81.02\n",
      "f1 score: 0.7495461388436626\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': 200, 'max_depth': 14, 'max_features': 0.8, 'max_samples': 0.95, 'class_weight': {0: 1, 1: 2}}\n",
    "\n",
    "best_data = {}\n",
    "best_data.update(pickle.load(open(folder_name + \"/\" + \"train_test_xy.dat\", \"rb\"))) \n",
    "best_data[\"ds_transf_train\"] = pickle.load(open(folder_name + \"/\" + \"ds_transf_train.dat\", \"rb\"))\n",
    "\n",
    "model = RandomForestClassifier(random_state=SEED,\n",
    "                               n_jobs=-1)\n",
    "model.set_params(**params)\n",
    "model.fit(best_data[\"X_train\"], best_data[\"y_train\"],\n",
    "          sample_weight=best_data[\"ds_transf_train\"].instance_weights)\n",
    "y_pred = model.predict(best_data[\"X_test\"])\n",
    "\n",
    "acc = round(accuracy_score(best_data[\"y_test\"],y_pred)*100, 2)\n",
    "cm = confusion_matrix(best_data[\"y_test\"], y_pred)\n",
    "c0 = round(cm[0,0]/np.sum(cm[0])*100, 2)\n",
    "c1 = round(cm[1,1]/np.sum(cm[1])*100, 2)\n",
    "report = classification_report(best_data[\"y_test\"], y_pred, output_dict=True)\n",
    "\n",
    "print(\"accuracy:\", acc)\n",
    "print(\"accuracy class 0:\", c0, \"accuracy class 1:\", c1)\n",
    "print(\"f1 score:\", report[\"macro avg\"][\"f1-score\"])\n",
    "\n",
    "dico_metrics = {}\n",
    "dico_metrics[\"acc\"] = acc\n",
    "dico_metrics[\"f1\"] = report[\"macro avg\"][\"f1-score\"]\n",
    "dico_metrics[\"c0\"] = c0\n",
    "dico_metrics[\"c1\"] = c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e449fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ================================================\n",
      "\n",
      "Protected: More vol\n",
      "------------------------------------\n",
      "accuracy :  78.16\n",
      "accuracy class 0:  69.55\n",
      "accuracy class 1:  84.98\n",
      "\n",
      "Protected: More prof\n",
      "------------------------------------\n",
      "accuracy :  70.6\n",
      "accuracy class 0:  71.62\n",
      "accuracy class 1:  67.18\n",
      "\n",
      "# ================================================\n",
      "\n",
      "Disparate_impact 1.6317883751240885\n",
      "\n",
      "# ================================================\n",
      "\n",
      "SEDF 0.48960472073187145\n"
     ]
    }
   ],
   "source": [
    "dfout = pd.DataFrame()\n",
    "dfout[\"ind_prof\"] = best_data[\"X_test\"][\"ind_prof\"]\n",
    "dfout[\"y_test\"] = best_data[\"y_test\"].values\n",
    "dfout[\"y_pred\"] = y_pred\n",
    "\n",
    "print(\"\\n# ================================================\")\n",
    "diff = 0\n",
    "for cat_name, cat_sex in [(\"More vol\", 0),\n",
    "                          (\"More prof\", 1)]:\n",
    "    print(\"\\nProtected:\", cat_name)\n",
    "    print(\"------------------------------------\")\n",
    "    dfaux = dfout.loc[dfout[\"ind_prof\"]==cat_sex]\n",
    "    acc = round(accuracy_score(dfaux[\"y_test\"].values, dfaux[\"y_pred\"].values)*100, 2)\n",
    "    cm = confusion_matrix(dfaux[\"y_test\"].values, dfaux[\"y_pred\"].values)\n",
    "    c0 = round(cm[0,0]/np.sum(cm[0])*100, 2)\n",
    "    c1 = round(cm[1,1]/np.sum(cm[1])*100, 2)\n",
    "    print(\"accuracy : \", acc)\n",
    "    print(\"accuracy class 0: \", c0)\n",
    "    print(\"accuracy class 1: \", c1)\n",
    "    dico_metrics[cat_name] = {\"acc\": acc,\n",
    "                              \"c0\": c0,\n",
    "                              \"c1\": c1,\n",
    "                             }\n",
    "    \n",
    "print(\"\\n# ================================================\")\n",
    "unpriv_df = dfout[dfout[\"ind_prof\"]==0] \n",
    "unpriv_total = unpriv_df.shape[0]\n",
    "\n",
    "priv_df = dfout[dfout[\"ind_prof\"]==1]\n",
    "priv_total = priv_df.shape[0]\n",
    "\n",
    "unpriv_outcomes = unpriv_df[unpriv_df[\"y_pred\"]==1].shape[0]\n",
    "unpriv_ratio = unpriv_outcomes/unpriv_total\n",
    "\n",
    "priv_outcomes = priv_df[priv_df[\"y_pred\"]==1].shape[0]\n",
    "priv_ratio = priv_outcomes/priv_total\n",
    "\n",
    "disparate_impact = unpriv_ratio/priv_ratio\n",
    "dico_metrics[\"disparate\"] = disparate_impact\n",
    "\n",
    "print(\"\\nDisparate_impact\", disparate_impact)\n",
    "\n",
    "print(\"\\n# ================================================\")\n",
    "privileged_groups = [{'ind_prof': 1}] \n",
    "unprivileged_groups = [{'ind_prof': 0}] \n",
    "ds = best_data['X_test'].copy()\n",
    "ds[\"y_pred\"] = y_pred\n",
    "binaryLabelDataset = BinaryLabelDataset(favorable_label=0,\n",
    "                                        unfavorable_label=1,\n",
    "                                        df=ds,\n",
    "                                        label_names=['y_pred'],\n",
    "                                        protected_attribute_names=['ind_prof'])\n",
    "metric = BinaryLabelDatasetMetric(binaryLabelDataset, \n",
    "                                 unprivileged_groups=unprivileged_groups,\n",
    "                                 privileged_groups=privileged_groups)\n",
    "print(\"\\nSEDF\", metric.smoothed_empirical_differential_fairness())\n",
    "dico_metrics[\"smoothed\"] = metric.smoothed_empirical_differential_fairness()\n",
    "pickle.dump(dico_metrics, open(folder_name + \"/\" + \"dico_metrics.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cac99d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
